## Complementary Exercise Texts


**Making Mona Lisa Smile** (Reconstructed by GPT from student commentary and direct quotes)

Imagine a computer that can tell exactly how you’re feeling—just by reading your face. It might sound like science fiction, but thanks to a new technology called the Facial Action Coding System (FACS), it’s now a reality.


This groundbreaking technology comes from the work of Professor Thomas Huang at the Beckman Institute for Advanced Science at the University of Illinois, in collaboration with Dr. Nicu Sebe of the University of Amsterdam. Their software allows computers to analyze and identify human emotions by tracking tiny facial movements. These experts are part of a field focused on building better ways for humans and computers to communicate.

The process starts when a computer builds a 3-D model of the human face. This model includes all 44 major facial muscles, each one moving like real human muscles. A movement of one or more muscles is called an “action unit.” These action units form the basis for emotional expression.

The system draws from the work of Dr. Paul Ekman, who identified six basic emotions:
happiness, surprise, anger, disgust, fear, and sadness.
Each of these emotions is associated with a unique combination of muscle movements. For instance, your frontalis pars lateralis raises your eyebrows when surprised, while your orbicularis oris tightens around the mouth when you’re angry.

These expressions are universal, notes Dr. Huang. Although people may show different intensities of emotion, the basic facial expressions appear across all cultures.

The researchers decided to put their system to the test by analyzing the most famously mysterious face in art—Leonardo da Vinci’s Mona Lisa. What is she really feeling? According to the system’s analysis:
“She’s 83 percent happy, 9 percent disgusted, 6 percent fearful, and 2 percent angry.”

This finding reveals that the Mona Lisa’s expression may contain mixed emotions, something previously difficult to define in a single image. The computer compares facial features to a “neutral face” to weigh different expressions and determine emotional states.

While the Mona Lisa analysis might bring a smile to your face, this technology has real-world applications, especially in classrooms. Dr. Huang predicts,
“A classroom computer could recognize when a student is becoming confused or bored. Then it could modify the lesson, like an effective human instructor.”

That means educational software could become adaptive, adjusting lessons based on students’ emotional feedback. It could also alert teachers when students feel frustrated, distracted, or disengaged—offering more chances for help.

In addition, this software might enhance video games, virtual surgeries, or even ads. For example,
“If you smile when a Web ad appears, a similar ad might follow. But if you frown, the next ad will be different.”

The system can also help spot fake smiles or forced emotions. It turns out that a genuine smile involves muscles near the eyes, like the orbicularis oculi, which creates "crow’s feet." Fake smiles, on the other hand, often only activate the zygomatic major, stretching the mouth sideways without engaging the eyes.

These clues are sometimes used to spot when a celebrity or politician isn’t being truthful, highlighting the potential for lie detection and behavioral analysis.

Though some may worry about privacy or accuracy, this technology opens up powerful possibilities. As Dr. Huang notes:
“Most human communication is nonverbal, including emotional communication. So computers need to understand that too.”

